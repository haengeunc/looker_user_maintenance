{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haengeunc/looker_user_maintenance/blob/main/Looker_user_maintenance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚡ Welcome to user maintenance tool! ⚡\n",
        "\n",
        "Do you want to automate a process of identifying and archiving user contents before deleting them from the Looker instance? This tool can help!\n",
        "\n",
        "Development environment requirements:\n",
        "*  looker-sdk-22.18.0\n",
        "*  looker-deployer\n",
        "*  ruby\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WQ3oKljdCBhN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up the Looker Python SDK"
      ],
      "metadata": {
        "id": "bofq65lQaOke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install looker-sdk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqMUz_40CF0c",
        "outputId": "f6ae2685-09ce-44c3-f13f-04e4d3017a32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: looker-sdk in /usr/local/lib/python3.7/dist-packages (22.18.0)\n",
            "Requirement already satisfied: attrs>=20.1.0 in /usr/local/lib/python3.7/dist-packages (from looker-sdk) (22.1.0)\n",
            "Requirement already satisfied: requests>=2.22 in /usr/local/lib/python3.7/dist-packages (from looker-sdk) (2.23.0)\n",
            "Requirement already satisfied: cattrs>=1.3 in /usr/local/lib/python3.7/dist-packages (from looker-sdk) (22.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from looker-sdk) (4.1.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.7/dist-packages (from cattrs>=1.3->looker-sdk) (1.0.0rc9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22->looker-sdk) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22->looker-sdk) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22->looker-sdk) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22->looker-sdk) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOp9JZxPm4pM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01e39eff-60ca-4dfd-c645-e54effe3c3d7"
      },
      "source": [
        "# For the SDK to work, you must either have set environment variables or created a looker.ini in the working directory.\n",
        "#set variables on the fly - note that API credentials should NOT be left if being shared. \n",
        "\n",
        "# 1. Setting environment variables:\n",
        "import os #We import os here in order to manage environment variables for the tutorial. You don't need to do this on a local system or anywhere you can more conveniently set environment variables.\n",
        "\n",
        "os.environ[\"LOOKERSDK_BASE_URL\"] = \"https://teach.corp.looker.com:19999\" #If your looker URL has .cloud in it (hosted on GCP), do not include :19999 (ie: https://your.cloud.looker.com).\n",
        "os.environ[\"LOOKERSDK_VERIFY_SSL\"] = \"true\" #Defaults to true if not set. SSL verification should generally be on unless you have a real good reason not to use it. Valid options: true, y, t, yes, 1.\n",
        "\n",
        "#Get the following values from your Users page in the Admin panel of your Looker instance > Users > Your user > Edit API keys. If you know your user id, you can visit https://your.looker.com/admin/users/<your_user_id>/edit.\n",
        "os.environ[\"LOOKERSDK_CLIENT_ID\"] =  \"x5FZYJnhHd4Ns4mxKpJT\" #No defaults.\n",
        "os.environ[\"LOOKERSDK_CLIENT_SECRET\"] = \"TqXRkMMskS6sfWPQHjHc9b4z\" #No defaults. This should be protected at all costs. Please do not leave it sitting here, even if you don't share this document.\n",
        "\n",
        "print(\"All environment variables set for\", os.environ[\"LOOKERSDK_BASE_URL\"])\n",
        "\n",
        "# 2. Alternaitve option is to create looker.ini file in the working directory:\n",
        "   #[Looker]\n",
        "   #base_url=https://teach.corp.looker.com:19999\n",
        "   #client_id=xxx\n",
        "   #client_secret=yyy\n",
        "   #verify_ssl=true\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All environment variables set for https://teach.corp.looker.com:19999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json #This is a handy library for doing JSON work."
      ],
      "metadata": {
        "id": "VHgdoK73dY1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_jEgRWjBcta",
        "outputId": "d1842845-c49a-4f01-cf90-53a1da43400e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Haengeun\n"
          ]
        }
      ],
      "source": [
        "import looker_sdk\n",
        "\n",
        "# For this to work you must either have set environment variables or created a looker.ini as described below in \"Configuring the SDK\"\n",
        "# Initialize Looker SDK 4.0\n",
        "sdk = looker_sdk.init40()  # or init31() for the older v3.1 API\n",
        "\n",
        "# Make your first Looker API calls\n",
        "my_user = sdk.me()\n",
        "\n",
        "print(my_user.first_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Looker deployer open source tool"
      ],
      "metadata": {
        "id": "II3iUM3paGJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Refer to https://github.com/looker-open-source/looker_deployer\n",
        "!pip install looker-deployer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVBy7AJoP7C8",
        "outputId": "f6eb4bfe-7ef6-42d2-b6d9-80691fa4bce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: looker-deployer in /usr/local/lib/python3.7/dist-packages (0.3.8)\n",
            "Requirement already satisfied: oyaml in /usr/local/lib/python3.7/dist-packages (from looker-deployer) (1.0)\n",
            "Requirement already satisfied: looker-sdk>=21.18.0 in /usr/local/lib/python3.7/dist-packages (from looker-deployer) (22.18.0)\n",
            "Requirement already satisfied: python-json-logger in /usr/local/lib/python3.7/dist-packages (from looker-deployer) (2.0.4)\n",
            "Requirement already satisfied: requests>=2.22 in /usr/local/lib/python3.7/dist-packages (from looker-sdk>=21.18.0->looker-deployer) (2.23.0)\n",
            "Requirement already satisfied: cattrs>=1.3 in /usr/local/lib/python3.7/dist-packages (from looker-sdk>=21.18.0->looker-deployer) (22.2.0)\n",
            "Requirement already satisfied: attrs>=20.1.0 in /usr/local/lib/python3.7/dist-packages (from looker-sdk>=21.18.0->looker-deployer) (22.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from looker-sdk>=21.18.0->looker-deployer) (4.1.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.7/dist-packages (from cattrs>=1.3->looker-sdk>=21.18.0->looker-deployer) (1.0.0rc9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22->looker-sdk>=21.18.0->looker-deployer) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22->looker-sdk>=21.18.0->looker-deployer) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22->looker-sdk>=21.18.0->looker-deployer) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22->looker-sdk>=21.18.0->looker-deployer) (1.24.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from oyaml->looker-deployer) (6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install ruby\n",
        "!sudo apt-get install ruby ruby-dev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RLNUKtFTt3a",
        "outputId": "15653785-e404-4f5c-d411-6d8d4445f487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ruby is already the newest version (1:2.5.1).\n",
            "ruby-dev is already the newest version (1:2.5.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 26 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gem install public_suffix -v 4.0.7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5IQmFslUTHJ",
        "outputId": "3eb14f38-ae00-4d11-d849-339b066bbe7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed public_suffix-4.0.7\n",
            "Parsing documentation for public_suffix-4.0.7\n",
            "Done installing documentation for public_suffix after 0 seconds\n",
            "1 gem installed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gem install gazer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2w0Sru8LUKTw",
        "outputId": "81c8d7c6-8541-4d8b-e525-83d92c3c1f3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed gazer-0.2.56\n",
            "Parsing documentation for gazer-0.2.56\n",
            "Done installing documentation for gazer after 0 seconds\n",
            "1 gem installed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "users_to_delete = [\"19371\", \"19811\"]\n",
        "\n",
        "#Loop through a list of users that need deleting and generate a list of folder numbers\n",
        "def get_folder_number():\n",
        "  \n",
        "  folder_id = []\n",
        "  \n",
        "  for uid in users_to_delete:\n",
        "    \n",
        "    response = sdk.user(\n",
        "            user_id= uid, #pass uid here ######***********\n",
        "            fields=\"email, personal_folder_id\")   \n",
        "    \n",
        "    folder_id.append(response['personal_folder_id'])\n",
        "  \n",
        "  return folder_id\n",
        "  \n",
        "  #print(response['personal_folder_id'])\n",
        "  #print(type(response['personal_folder_id']))\n",
        "\n",
        "#run function to print folder ID\n",
        "folder_list = get_folder_number()\n",
        "print(folder_list)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuW6cleLZ2nc",
        "outputId": "204f7af6-81ce-465d-ba08-be1211dc1e1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['16640', '16986']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list = ''\n",
        "for item in folder_list:\n",
        "  list = list + ' ' + item\n",
        "\n",
        "print(list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEMjZKVWOiHT",
        "outputId": "21dd4722-cc51-4610-dd7c-820ed3382b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 16640 16986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ldeploy content export -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7xtziriz5m6",
        "outputId": "a3d0affe-a7ea-4103-e040-f0a3d0f596dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: ldeploy content export [-h] --env ENV [--ini INI] [--debug] --folders\n",
            "                              FOLDERS [FOLDERS ...] --local-target\n",
            "                              LOCAL_TARGET\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --env ENV             What environment to deploy to\n",
            "  --ini INI             ini file to parse for credentials\n",
            "  --debug               set logger to debug for more verbosity\n",
            "  --folders FOLDERS [FOLDERS ...]\n",
            "                        What folders to export content from\n",
            "  --local-target LOCAL_TARGET\n",
            "                        Local directory to store content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#run ldeploy to export a content from a specified folder to local disk\n",
        "!ldeploy content export --env Looker --folders $list --local-target ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3GKEccVUXLN",
        "outputId": "0bcce34a-3dee-439d-9fe9-ab6d128feb59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"levelname\": \"INFO\", \"module\": \"deploy_content_export\", \"funcName\": \"main\", \"message\": \"Exporting content\", \"env\": \"ENV\", \"folders\": [\"16640\", \"16986\"], \"local_target\": \".\", \"timestamp\": \"2022-10-24T16:36:37.383794Z\"}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/ldeploy\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/looker_deployer/cli.py\", line 46, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/looker_deployer/commands/deploy_content_export.py\", line 105, in main\n",
            "    sdk = get_client(args.ini, args.env)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/looker_deployer/utils/get_client.py\", line 42, in get_client\n",
            "    sdk = configure_sdk(config_file=ini, section=env)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/looker_deployer/utils/get_client.py\", line 27, in configure_sdk\n",
            "    _settings(config_file, section) if config_settings is None else config_settings\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/looker_sdk/__init__.py\", line 47, in _settings\n",
            "    env_prefix=constants.environment_prefix,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/looker_sdk/rtl/api_settings.py\", line 94, in __init__\n",
            "    data = self.read_config()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/looker_sdk/rtl/api_settings.py\", line 123, in read_config\n",
            "    raise cp.NoSectionError(section)\n",
            "configparser.NoSectionError: No section: 'ENV'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a Google Cloud Storage (gcs) bucket to save folder contents"
      ],
      "metadata": {
        "id": "_4_U6kmjXsCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#authenticate with google cloud sdk\n",
        "!gcloud auth login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KRdNGryUvlb",
        "outputId": "53211e5c-7197-4b2c-cca1-8154ebce9ea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go to the following link in your browser:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=FzCx2wj49181GOo9DsqauRzAo2x4cM&prompt=consent&access_type=offline&code_challenge=dKjA_ns18H1zP6wZt9TyxnOdqC-9xUzE9lip-Rtw2YI&code_challenge_method=S256\n",
            "\n",
            "Enter authorization code: 4/0ARtbsJoFnGa21uHSQvl8-FAGxYQ8MZ6SA7Y1IO1CfZqQYbXYV8b2lIwoTyh5iyZPQr9ISA\n",
            "\n",
            "You are now logged in as [haengeun@google.com].\n",
            "Your current project is [haengeun].  You can change this setting by running:\n",
            "  $ gcloud config set project PROJECT_ID\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#set project ID in xxx\n",
        "#!gcloud config set project xxx\n",
        "!gcloud config set project haengeun"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwM-CI8zXpcM",
        "outputId": "b3c65860-a66b-4a12-e1f9-a47e09906e6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#list all the cloud storage buckets available\n",
        "!gsutil ls "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLYp4heoY0s7",
        "outputId": "ec59ee67-4130-4e5f-afda-6e8e3630bd71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gs://dataproc-staging-us-central1-1048293672564-jdjrxdix/\n",
            "gs://dataproc-temp-us-central1-1048293672564-lgqq5ibd/\n",
            "gs://e4-terraform-bucket/\n",
            "gs://haengeun-bucket2/\n",
            "gs://teach-instance-maintenance/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cloud storage - Copy files and objects into \n",
        "#gsutil cp -r dir gs://my-bucket/data\n",
        "\n",
        "!gsutil -m cp -r Users gs://teach-instance-maintenance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3W9F9JTauVL",
        "outputId": "0e144022-d44d-4fd1-ed21-5b5f722fbe9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file://Users/Haengeun BI on GCP/Dashboard_5139_Test dashboard.json [Content-Type=application/json]...\n",
            "Copying file://Users/Haengeun (Teach Global Liberty)/Space_16640_Haengeun (Teach Global Liberty).json [Content-Type=application/json]...\n",
            "/ [0/8 files][    0.0 B/ 46.3 KiB]   0% Done                                    \r/ [0/8 files][    0.0 B/ 46.3 KiB]   0% Done                                    \rCopying file://Users/Haengeun BI on GCP/Space_16706_Haengeun BI on GCP.json [Content-Type=application/json]...\n",
            "/ [0/8 files][    0.0 B/ 46.3 KiB]   0% Done                                    \rCopying file://Users/Amine Hakkou/Space_14439_Amine Hakkou.json [Content-Type=application/json]...\n",
            "/ [0/8 files][    0.0 B/ 46.3 KiB]   0% Done                                    \rCopying file://Users/Karen Pang/Space_16986_Karen Pang.json [Content-Type=application/json]...\n",
            "/ [0/8 files][    0.0 B/ 46.3 KiB]   0% Done                                    \rCopying file://Users/Haengeun (Teach Global Liberty)/Dashboard_5094_New Dashboard.json [Content-Type=application/json]...\n",
            "/ [0/8 files][    0.0 B/ 46.3 KiB]   0% Done                                    \rCopying file://Users/Haengeun Looker/Look_12958_Best selling items.json [Content-Type=application/json]...\n",
            "Copying file://Users/Haengeun Looker/Space_16681_Haengeun Looker.json [Content-Type=application/json]...\n",
            "/ [8/8 files][ 46.3 KiB/ 46.3 KiB] 100% Done                                    \n",
            "Operation completed over 8 objects/46.3 KiB.                                     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil help cp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BShuiil1sLHU",
        "outputId": "89d1b951-09f1-4027-c96a-eec06f0b9305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mNAME\u001b[0;0m\n",
            "  cp - Copy files and objects\n",
            "\n",
            "\n",
            "\u001b[1mSYNOPSIS\u001b[0;0m\n",
            "\n",
            "  gsutil cp [OPTION]... src_url dst_url\n",
            "  gsutil cp [OPTION]... src_url... dst_url\n",
            "  gsutil cp [OPTION]... -I dst_url\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mDESCRIPTION\u001b[0;0m\n",
            "  The ``gsutil cp`` command allows you to copy data between your local file\n",
            "  system and the cloud, within the cloud, and between\n",
            "  cloud storage providers. For example, to upload all text files from the\n",
            "  local directory to a bucket, you can run:\n",
            "\n",
            "    gsutil cp *.txt gs://my-bucket\n",
            "\n",
            "  You can also download data from a bucket. The following command downloads\n",
            "  all text files from the top-level of a bucket to your current directory:\n",
            "\n",
            "    gsutil cp gs://my-bucket/*.txt .\n",
            "\n",
            "  You can use the ``-n`` option to prevent overwriting the content of\n",
            "  existing files. The following example downloads text files from a bucket\n",
            "  without clobbering the data in your directory:\n",
            "\n",
            "    gsutil cp -n gs://my-bucket/*.txt .\n",
            "\n",
            "  Use the ``-r`` option to copy an entire directory tree.\n",
            "  For example, to upload the directory tree ``dir``:\n",
            "\n",
            "    gsutil cp -r dir gs://my-bucket\n",
            "\n",
            "  If you have a large number of files to transfer, you can perform a parallel\n",
            "  multi-threaded/multi-processing copy using the\n",
            "  top-level gsutil ``-m`` option (see \"gsutil help options\"):\n",
            "\n",
            "    gsutil -m cp -r dir gs://my-bucket\n",
            "\n",
            "  You can use the ``-I`` option with ``stdin`` to specify a list of URLs to\n",
            "  copy, one per line. This allows you to use gsutil\n",
            "  in a pipeline to upload or download objects as generated by a program:\n",
            "\n",
            "    cat filelist | gsutil -m cp -I gs://my-bucket\n",
            "\n",
            "  or:\n",
            "\n",
            "    cat filelist | gsutil -m cp -I ./download_dir\n",
            "\n",
            "  where the output of ``cat filelist`` is a list of files, cloud URLs, and\n",
            "  wildcards of files and cloud URLs.\n",
            "\n",
            "  NOTE: Shells like ``bash`` and ``zsh`` sometimes attempt to expand\n",
            "  wildcards in ways that can be surprising. You may also encounter issues when\n",
            "  attempting to copy files whose names contain wildcard characters. For more\n",
            "  details about these issues, see `Wildcard behavior considerations\n",
            "  <https://cloud.google.com/storage/docs/wildcards#surprising-behavior>`_.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mHOW NAMES ARE CONSTRUCTED\u001b[0;0m\n",
            "  The ``gsutil cp`` command attempts to name objects in ways that are consistent with the\n",
            "  Linux ``cp`` command. This means that names are constructed depending\n",
            "  on whether you're performing a recursive directory copy or copying\n",
            "  individually-named objects, or whether you're copying to an existing or\n",
            "  non-existent directory.\n",
            "\n",
            "  When you perform recursive directory copies, object names are constructed to\n",
            "  mirror the source directory structure starting at the point of recursive\n",
            "  processing. For example, if ``dir1/dir2`` contains the file ``a/b/c``, then the\n",
            "  following command creates the object ``gs://my-bucket/dir2/a/b/c``:\n",
            "\n",
            "    gsutil cp -r dir1/dir2 gs://my-bucket\n",
            "\n",
            "  In contrast, copying individually-named files results in objects named by\n",
            "  the final path component of the source files. For example, assuming again that\n",
            "  ``dir1/dir2`` contains ``a/b/c``, the following command creates the object\n",
            "  ``gs://my-bucket/c``:\n",
            "\n",
            "    gsutil cp dir1/dir2/** gs://my-bucket\n",
            "\n",
            "  Note that in the above example, the '**' wildcard matches all names\n",
            "  anywhere under ``dir``. The wildcard '*' matches names just one level deep. For\n",
            "  more details, see `URI wildcards\n",
            "  <https://cloud.google.com/storage/docs/wildcards#surprising-behavior>`_.\n",
            "\n",
            "  The same rules apply for uploads and downloads: recursive copies of buckets and\n",
            "  bucket subdirectories produce a mirrored filename structure, while copying\n",
            "  individually or wildcard-named objects produce flatly-named files.\n",
            "\n",
            "  In addition, the resulting names depend on whether the destination subdirectory\n",
            "  exists. For example, if ``gs://my-bucket/subdir`` exists as a subdirectory,\n",
            "  the following command creates the object ``gs://my-bucket/subdir/dir2/a/b/c``:\n",
            "\n",
            "    gsutil cp -r dir1/dir2 gs://my-bucket/subdir\n",
            "\n",
            "  In contrast, if ``gs://my-bucket/subdir`` does not exist, this same ``gsutil cp``\n",
            "  command creates the object ``gs://my-bucket/subdir/a/b/c``.\n",
            "\n",
            "  NOTE: The\n",
            "  `Google Cloud Platform Console <https://console.cloud.google.com>`_\n",
            "  creates folders by creating \"placeholder\" objects that end\n",
            "  with a \"/\" character. gsutil skips these objects when downloading from the\n",
            "  cloud to the local file system, because creating a file that\n",
            "  ends with a \"/\" is not allowed on Linux and macOS. We\n",
            "  recommend that you only create objects that end with \"/\" if you don't\n",
            "  intend to download such objects using gsutil.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mCOPYING TO/FROM SUBDIRECTORIES; DISTRIBUTING TRANSFERS ACROSS MACHINES\u001b[0;0m\n",
            "  You can use gsutil to copy to and from subdirectories by using a command\n",
            "  like this:\n",
            "\n",
            "    gsutil cp -r dir gs://my-bucket/data\n",
            "\n",
            "  This causes ``dir`` and all of its files and nested subdirectories to be\n",
            "  copied under the specified destination, resulting in objects with names like\n",
            "  ``gs://my-bucket/data/dir/a/b/c``. Similarly, you can download from bucket\n",
            "  subdirectories using the following command:\n",
            "\n",
            "    gsutil cp -r gs://my-bucket/data dir\n",
            "\n",
            "  This causes everything nested under ``gs://my-bucket/data`` to be downloaded\n",
            "  into ``dir``, resulting in files with names like ``dir/data/a/b/c``.\n",
            "\n",
            "  Copying subdirectories is useful if you want to add data to an existing\n",
            "  bucket directory structure over time. It's also useful if you want\n",
            "  to parallelize uploads and downloads across multiple machines (potentially\n",
            "  reducing overall transfer time compared with running ``gsutil -m\n",
            "  cp`` on one machine). For example, if your bucket contains this structure:\n",
            "\n",
            "    gs://my-bucket/data/result_set_01/\n",
            "    gs://my-bucket/data/result_set_02/\n",
            "    ...\n",
            "    gs://my-bucket/data/result_set_99/\n",
            "\n",
            "  you can perform concurrent downloads across 3 machines by running these\n",
            "  commands on each machine, respectively:\n",
            "\n",
            "    gsutil -m cp -r gs://my-bucket/data/result_set_[0-3]* dir\n",
            "    gsutil -m cp -r gs://my-bucket/data/result_set_[4-6]* dir\n",
            "    gsutil -m cp -r gs://my-bucket/data/result_set_[7-9]* dir\n",
            "\n",
            "  Note that ``dir`` could be a local directory on each machine, or a\n",
            "  directory mounted off of a shared file server. The performance of the latter\n",
            "  depends on several factors, so we recommend experimenting\n",
            "  to find out what works best for your computing environment.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mCOPYING IN THE CLOUD AND METADATA PRESERVATION\u001b[0;0m\n",
            "  If both the source and destination URL are cloud URLs from the same\n",
            "  provider, gsutil copies data \"in the cloud\" (without downloading\n",
            "  to and uploading from the machine where you run gsutil). In addition to\n",
            "  the performance and cost advantages of doing this, copying in the cloud\n",
            "  preserves metadata such as ``Content-Type`` and ``Cache-Control``. In contrast,\n",
            "  when you download data from the cloud, it ends up in a file with\n",
            "  no associated metadata, unless you have some way to keep\n",
            "  or re-create that metadata.\n",
            "\n",
            "  Copies spanning locations and/or storage classes cause data to be rewritten\n",
            "  in the cloud, which may take some time (but is still faster than\n",
            "  downloading and re-uploading). Such operations can be resumed with the same\n",
            "  command if they are interrupted, so long as the command parameters are\n",
            "  identical.\n",
            "\n",
            "  Note that by default, the gsutil ``cp`` command does not copy the object\n",
            "  ACL to the new object, and instead uses the default bucket ACL (see\n",
            "  \"gsutil help defacl\"). You can override this behavior with the ``-p``\n",
            "  option.\n",
            "\n",
            "  When copying in the cloud, if the destination bucket has Object Versioning\n",
            "  enabled, by default ``gsutil cp`` copies only live versions of the\n",
            "  source object. For example, the following command causes only the single live\n",
            "  version of ``gs://bucket1/obj`` to be copied to ``gs://bucket2``, even if there\n",
            "  are noncurrent versions of ``gs://bucket1/obj``:\n",
            "\n",
            "    gsutil cp gs://bucket1/obj gs://bucket2\n",
            "\n",
            "  To also copy noncurrent versions, use the ``-A`` flag:\n",
            "\n",
            "    gsutil cp -A gs://bucket1/obj gs://bucket2\n",
            "\n",
            "  The top-level gsutil ``-m`` flag is  not allowed when using the ``cp -A`` flag.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mCHECKSUM VALIDATION\u001b[0;0m\n",
            "  At the end of every upload or download, the ``gsutil cp`` command validates that\n",
            "  the checksum it computes for the source file matches the checksum that\n",
            "  the service computes. If the checksums do not match, gsutil deletes the\n",
            "  corrupted object and prints a warning message. If this happens, contact\n",
            "  gs-team@google.com.\n",
            "\n",
            "  If you know the MD5 of a file before uploading, you can specify it in the\n",
            "  Content-MD5 header, which enables the cloud storage service to reject the\n",
            "  upload if the MD5 doesn't match the value computed by the service. For\n",
            "  example:\n",
            "\n",
            "    % gsutil hash obj\n",
            "    Hashing     obj:\n",
            "    Hashes [base64] for obj:\n",
            "            Hash (crc32c):          lIMoIw==\n",
            "            Hash (md5):             VgyllJgiiaRAbyUUIqDMmw==\n",
            "\n",
            "    % gsutil -h Content-MD5:VgyllJgiiaRAbyUUIqDMmw== cp obj gs://your-bucket/obj\n",
            "    Copying file://obj [Content-Type=text/plain]...\n",
            "    Uploading   gs://your-bucket/obj:                                182 b/182 B\n",
            "\n",
            "  If the checksums don't match, the service rejects the upload and\n",
            "  gsutil prints a message like:\n",
            "\n",
            "    BadRequestException: 400 Provided MD5 hash \"VgyllJgiiaRAbyUUIqDMmw==\"\n",
            "    doesn't match calculated MD5 hash \"7gyllJgiiaRAbyUUIqDMmw==\".\n",
            "\n",
            "  Specifying the Content-MD5 header has several advantages:\n",
            "\n",
            "  1. It prevents the corrupted object from becoming visible. If you don't\n",
            "     specify the header, the object is visible for 1-3 seconds before gsutil deletes\n",
            "     it.\n",
            "\n",
            "  2. If an object already exists with the given name, specifying the\n",
            "     Content-MD5 header prevents the existing object from being replaced.\n",
            "     Otherwise, the existing object is replaced by the corrupted object and\n",
            "     deleted a few seconds later.\n",
            "\n",
            "  3. If you don't specify the Content-MD5 header, it's possible for the gsutil\n",
            "     process to complete the upload but then be interrupted or fail before it can\n",
            "     delete the corrupted object, leaving the corrupted object in the cloud.\n",
            "\n",
            "  4. It supports a customer-to-service integrity check handoff. For example,\n",
            "     if you have a content production pipeline that generates data to be\n",
            "     uploaded to the cloud along with checksums of that data, specifying the\n",
            "     MD5 computed by your content pipeline when you run ``gsutil cp`` ensures\n",
            "     that the checksums match all the way through the process. This way, you can\n",
            "     detect if data gets corrupted on your local disk between the time it was written\n",
            "     by your content pipeline and the time it was uploaded to Google Cloud\n",
            "     Storage.\n",
            "\n",
            "  NOTE: The Content-MD5 header is ignored for composite objects, which only have\n",
            "  a CRC32C checksum.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mRETRY HANDLING\u001b[0;0m\n",
            "  The ``cp`` command retries when failures occur, but if enough failures happen\n",
            "  during a particular copy or delete operation, or if a failure isn't retryable,\n",
            "  the ``cp`` command skips that object and moves on. If any failures were not\n",
            "  successfully retried by the end of the copy run, the ``cp`` command reports the\n",
            "  number of failures, and exits with a non-zero status.\n",
            "\n",
            "  For details about gsutil's overall retry handling, see `Retry strategy\n",
            "  <https://cloud.google.com/storage/docs/retry-strategy#tools>`_.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mRESUMABLE TRANSFERS\u001b[0;0m\n",
            "  gsutil automatically resumes interrupted downloads and interrupted `resumable\n",
            "  uploads <https://cloud.google.com/storage/docs/resumable-uploads#gsutil>`_,\n",
            "  except when performing streaming transfers. In the case of an interrupted\n",
            "  download, a partially downloaded temporary file is visible in the destination\n",
            "  directory. Upon completion, the original file is deleted and replaced with the\n",
            "  downloaded contents.\n",
            "\n",
            "  Resumable transfers store state information in files under\n",
            "  ~/.gsutil, named by the destination object or file.\n",
            "\n",
            "  See \"gsutil help prod\" for details on using resumable transfers\n",
            "  in production.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mSTREAMING TRANSFERS\u001b[0;0m\n",
            "  Use '-' in place of src_url or dst_url to perform a `streaming transfer\n",
            "  <https://cloud.google.com/storage/docs/streaming>`_.\n",
            "\n",
            "  Streaming uploads using the `JSON API\n",
            "  <https://cloud.google.com/storage/docs/request-endpoints#gsutil>`_ are buffered\n",
            "  in memory part-way back into the file and can thus sometimes resume in the event\n",
            "  of network or service problems.\n",
            "\n",
            "  gsutil does not support resuming streaming uploads using the XML API or\n",
            "  resuming streaming downloads for either JSON or XML. If you have a large amount\n",
            "  of data to transfer in these cases, we recommend that you write the data to a\n",
            "  local file and copy that file rather than streaming it.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mSLICED OBJECT DOWNLOADS\u001b[0;0m\n",
            "  gsutil can automatically use ranged ``GET`` requests to perform downloads in\n",
            "  parallel for large files being downloaded from Cloud Storage. See `sliced object\n",
            "  download documentation\n",
            "  <https://cloud.google.com/storage/docs/sliced-object-downloads>`_\n",
            "  for a complete discussion.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mPARALLEL COMPOSITE UPLOADS\u001b[0;0m\n",
            "  gsutil can automatically use\n",
            "  `object composition <https://cloud.google.com/storage/docs/composite-objects>`_\n",
            "  to perform uploads in parallel for large, local files being uploaded to\n",
            "  Cloud Storage. See the `parallel composite uploads documentation\n",
            "  <https://cloud.google.com/storage/docs/parallel-composite-uploads>`_ for a\n",
            "  complete discussion.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mCHANGING TEMP DIRECTORIES\u001b[0;0m\n",
            "  gsutil writes data to a temporary directory in several cases:\n",
            "\n",
            "  - when compressing data to be uploaded (see the ``-z`` and ``-Z`` options)\n",
            "  - when decompressing data being downloaded (for example, when the data has\n",
            "    ``Content-Encoding:gzip`` as a result of being uploaded\n",
            "    using gsutil cp -z or gsutil cp -Z)\n",
            "  - when running integration tests using the gsutil test command\n",
            "\n",
            "  In these cases, it's possible the temporary file location on your system that\n",
            "  gsutil selects by default may not have enough space. If gsutil runs out of\n",
            "  space during one of these operations (for example, raising\n",
            "  \"CommandException: Inadequate temp space available to compress <your file>\"\n",
            "  during a ``gsutil cp -z`` operation), you can change where it writes these\n",
            "  temp files by setting the TMPDIR environment variable. On Linux and macOS,\n",
            "  you can set the variable as follows:\n",
            "\n",
            "    TMPDIR=/some/directory gsutil cp ...\n",
            "\n",
            "  You can also add this line to your ~/.bashrc file and restart the shell\n",
            "  before running gsutil:\n",
            "\n",
            "    export TMPDIR=/some/directory\n",
            "\n",
            "  On Windows 7, you can change the TMPDIR environment variable from Start ->\n",
            "  Computer -> System -> Advanced System Settings -> Environment Variables.\n",
            "  You need to reboot after making this change for it to take effect. Rebooting\n",
            "  is not necessary after running the export command on Linux and macOS.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mSYNCHRONIZING OVER OS-SPECIFIC FILE TYPES (SUCH AS SYMLINKS AND DEVICES)\u001b[0;0m\n",
            "\n",
            "  Please see the section about OS-specific file types in \"gsutil help rsync\".\n",
            "  While that section refers to the ``rsync`` command, analogous\n",
            "  points apply to the ``cp`` command.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mOPTIONS\u001b[0;0m\n",
            "  -a canned_acl  Applies the specific ``canned_acl`` to uploaded objects. See\n",
            "                 \"gsutil help acls\" for further details.\n",
            "\n",
            "  -A             Copy all source versions from a source bucket or folder.\n",
            "                 If not set, only the live version of each source object is\n",
            "                 copied.\n",
            "\n",
            "                 NOTE: This option is only useful when the destination\n",
            "                 bucket has Object Versioning enabled. Additionally, the generation\n",
            "                 numbers of copied versions do not necessarily match the order of the\n",
            "                 original generation numbers.\n",
            "\n",
            "  -c             If an error occurs, continue attempting to copy the remaining\n",
            "                 files. If any copies are unsuccessful, gsutil's exit status\n",
            "                 is non-zero, even if this flag is set. This option is\n",
            "                 implicitly set when running ``gsutil -m cp...``.\n",
            "\n",
            "                 NOTE: ``-c`` only applies to the actual copying operation. If an\n",
            "                 error, such as ``invalid Unicode file name``, occurs while iterating\n",
            "                 over the files in the local directory, gsutil prints an error\n",
            "                 message and aborts.\n",
            "\n",
            "  -D             Copy in \"daisy chain\" mode, which means copying between two buckets\n",
            "                 by first downloading to the machine where gsutil is run, then\n",
            "                 uploading to the destination bucket. The default mode is a\n",
            "                 \"copy in the cloud,\" where data is copied between two buckets without\n",
            "                 uploading or downloading.\n",
            "\n",
            "                 During a \"copy in the cloud,\" a source composite object remains composite\n",
            "                 at its destination. However, you can use \"daisy chain\" mode to change a\n",
            "                 composite object into a non-composite object. For example:\n",
            "\n",
            "                     gsutil cp -D gs://bucket/obj gs://bucket/obj_tmp\n",
            "                     gsutil mv gs://bucket/obj_tmp gs://bucket/obj\n",
            "\n",
            "                 NOTE: \"Daisy chain\" mode is automatically used when copying\n",
            "                 between providers: for example, when copying data from Cloud Storage\n",
            "                 to another provider.\n",
            "\n",
            "  -e             Exclude symlinks. When specified, symbolic links are not copied.\n",
            "\n",
            "  -I             Use ``stdin`` to specify a list of files or objects to copy. You can use\n",
            "                 gsutil in a pipeline to upload or download objects as generated by a program.\n",
            "                 For example:\n",
            "\n",
            "                   cat filelist | gsutil -m cp -I gs://my-bucket\n",
            "\n",
            "                 where the output of ``cat filelist`` is a one-per-line list of\n",
            "                 files, cloud URLs, and wildcards of files and cloud URLs.\n",
            "\n",
            "  -j <ext,...>   Applies gzip transport encoding to any file upload whose\n",
            "                 extension matches the ``-j`` extension list. This is useful when\n",
            "                 uploading files with compressible content such as .js, .css,\n",
            "                 or .html files. This also saves network bandwidth while\n",
            "                 leaving the data uncompressed in Cloud Storage.\n",
            "\n",
            "                 When you specify the ``-j`` option, files being uploaded are\n",
            "                 compressed in-memory and on-the-wire only. Both the local\n",
            "                 files and Cloud Storage objects remain uncompressed. The\n",
            "                 uploaded objects retain the ``Content-Type`` and name of the\n",
            "                 original files.\n",
            "\n",
            "                 Note that if you want to use the top-level ``-m`` option to\n",
            "                 parallelize copies along with the ``-j/-J`` options, your\n",
            "                 performance may be bottlenecked by the\n",
            "                 \"max_upload_compression_buffer_size\" boto config option,\n",
            "                 which is set to 2 GiB by default. You can change this\n",
            "                 compression buffer size to a higher limit. For example:\n",
            "\n",
            "                   gsutil -o \"GSUtil:max_upload_compression_buffer_size=8G\" \\\n",
            "                     -m cp -j html,txt -r /local/source/dir gs://bucket/path\n",
            "\n",
            "  -J             Applies gzip transport encoding to file uploads. This option\n",
            "                 works like the ``-j`` option described above, but it applies to\n",
            "                 all uploaded files, regardless of extension.\n",
            "\n",
            "                 CAUTION: If some of the source files don't compress well, such\n",
            "                 as binary data, using this option may result in longer uploads.\n",
            "\n",
            "  -L <file>      Outputs a manifest log file with detailed information about\n",
            "                 each item that was copied. This manifest contains the following\n",
            "                 information for each item:\n",
            "\n",
            "                 - Source path.\n",
            "                 - Destination path.\n",
            "                 - Source size.\n",
            "                 - Bytes transferred.\n",
            "                 - MD5 hash.\n",
            "                 - Transfer start time and date in UTC and ISO 8601 format.\n",
            "                 - Transfer completion time and date in UTC and ISO 8601 format.\n",
            "                 - Upload id, if a resumable upload was performed.\n",
            "                 - Final result of the attempted transfer, either success or failure.\n",
            "                 - Failure details, if any.\n",
            "\n",
            "                 If the log file already exists, gsutil uses the file as an\n",
            "                 input to the copy process, and appends log items to\n",
            "                 the existing file. Objects that are marked in the\n",
            "                 existing log file as having been successfully copied or\n",
            "                 skipped are ignored. Objects without entries are\n",
            "                 copied and ones previously marked as unsuccessful are\n",
            "                 retried. This option can be used in conjunction with the ``-c`` option to\n",
            "                 build a script that copies a large number of objects reliably,\n",
            "                 using a bash script like the following:\n",
            "\n",
            "                   until gsutil cp -c -L cp.log -r ./dir gs://bucket; do\n",
            "                     sleep 1\n",
            "                   done\n",
            "\n",
            "                 The -c option enables copying to continue after failures\n",
            "                 occur, and the -L option allows gsutil to pick up where it\n",
            "                 left off without duplicating work. The loop continues\n",
            "                 running as long as gsutil exits with a non-zero status. A non-zero\n",
            "                 status indicates there was at least one failure during the copy\n",
            "                 operation.\n",
            "\n",
            "                 NOTE: If you are synchronizing the contents of a\n",
            "                 directory and a bucket, or the contents of two buckets, see\n",
            "                 \"gsutil help rsync\".\n",
            "\n",
            "  -n             No-clobber. When specified, existing files or objects at the\n",
            "                 destination are not replaced. Any items that are skipped\n",
            "                 by this option are reported as skipped. gsutil\n",
            "                 performs an additional GET request to check if an item\n",
            "                 exists before attempting to upload the data. This saves gsutil\n",
            "                 from retransmitting data, but the additional HTTP requests may make\n",
            "                 small object transfers slower and more expensive.\n",
            "\n",
            "  -p             Preserves ACLs when copying in the cloud. Note\n",
            "                 that this option has performance and cost implications only when\n",
            "                 using the XML API, as the XML API requires separate HTTP calls for\n",
            "                 interacting with ACLs. You can mitigate this\n",
            "                 performance issue using ``gsutil -m cp`` to perform parallel\n",
            "                 copying. Note that this option only works if you have OWNER access\n",
            "                 to all objects that are copied. If you want all objects in the\n",
            "                 destination bucket to end up with the same ACL, you can avoid these\n",
            "                 performance issues by setting a default object ACL on that bucket\n",
            "                 instead of using ``cp -p``. See \"gsutil help defacl\".\n",
            "\n",
            "                 Note that it's not valid to specify both the ``-a`` and ``-p`` options\n",
            "                 together.\n",
            "\n",
            "  -P             Enables POSIX attributes to be preserved when objects are\n",
            "                 copied. ``gsutil cp`` copies fields provided by ``stat``. These fields\n",
            "                 are the user ID of the owner, the group\n",
            "                 ID of the owning group, the mode or permissions of the file, and\n",
            "                 the access and modification time of the file. For downloads, these\n",
            "                 attributes are only set if the source objects were uploaded\n",
            "                 with this flag enabled.\n",
            "\n",
            "                 On Windows, this flag only sets and restores access time and\n",
            "                 modification time. This is because Windows doesn't support\n",
            "                 POSIX uid/gid/mode.\n",
            "\n",
            "  -R, -r         The ``-R`` and ``-r`` options are synonymous. They enable directories,\n",
            "                 buckets, and bucket subdirectories to be copied recursively.\n",
            "                 If you don't use this option for an upload, gsutil copies objects\n",
            "                 it finds and skips directories. Similarly, if you don't\n",
            "                 specify this option for a download, gsutil copies\n",
            "                 objects at the current bucket directory level and skips subdirectories.\n",
            "\n",
            "  -s <class>     Specifies the storage class of the destination object. If not\n",
            "                 specified, the default storage class of the destination bucket\n",
            "                 is used. This option is not valid for copying to non-cloud destinations.\n",
            "\n",
            "  -U             Skips objects with unsupported object types instead of failing.\n",
            "                 Unsupported object types include Amazon S3 objects in the GLACIER\n",
            "                 storage class.\n",
            "\n",
            "  -v             Prints the version-specific URL for each uploaded object. You can\n",
            "                 use these URLs to safely make concurrent upload requests, because\n",
            "                 Cloud Storage refuses to perform an update if the current\n",
            "                 object version doesn't match the version-specific URL. See\n",
            "                 `generation numbers\n",
            "                 <https://cloud.google.com/storage/docs/metadata#generation-number>`_\n",
            "                 for more details.\n",
            "\n",
            "  -z <ext,...>   Applies gzip content-encoding to any file upload whose\n",
            "                 extension matches the ``-z`` extension list. This is useful when\n",
            "                 uploading files with compressible content such as .js, .css,\n",
            "                 or .html files, because it reduces network bandwidth and storage\n",
            "                 sizes. This can both improve performance and reduce costs.\n",
            "\n",
            "                 When you specify the ``-z`` option, the data from your files is\n",
            "                 compressed before it is uploaded, but your actual files are\n",
            "                 left uncompressed on the local disk. The uploaded objects\n",
            "                 retain the ``Content-Type`` and name of the original files, but\n",
            "                 have their ``Content-Encoding`` metadata set to ``gzip`` to\n",
            "                 indicate that the object data stored are compressed on the\n",
            "                 Cloud Storage servers and have their ``Cache-Control`` metadata\n",
            "                 set to ``no-transform``.\n",
            "\n",
            "                 For example, the following command:\n",
            "\n",
            "                   gsutil cp -z html \\\n",
            "                     cattypes.html tabby.jpeg gs://mycats\n",
            "\n",
            "                 does the following:\n",
            "\n",
            "                 - The ``cp`` command uploads the files ``cattypes.html`` and\n",
            "                   ``tabby.jpeg`` to the bucket ``gs://mycats``.\n",
            "                 - Based on the file extensions, gsutil sets the ``Content-Type``\n",
            "                   of ``cattypes.html`` to ``text/html`` and ``tabby.jpeg`` to\n",
            "                   ``image/jpeg``.\n",
            "                 - The ``-z`` option compresses the data in the file ``cattypes.html``.\n",
            "                 - The ``-z`` option also sets the ``Content-Encoding`` for\n",
            "                   ``cattypes.html`` to ``gzip`` and the ``Cache-Control`` for\n",
            "                   ``cattypes.html`` to ``no-transform``.\n",
            "\n",
            "                 Because the ``-z/-Z`` options compress data prior to upload, they\n",
            "                 are not subject to the same compression buffer bottleneck that\n",
            "                 can affect the ``-j/-J`` options.\n",
            "\n",
            "                 Note that if you download an object with ``Content-Encoding:gzip``,\n",
            "                 gsutil decompresses the content before writing the local file.\n",
            "\n",
            "  -Z             Applies gzip content-encoding to file uploads. This option\n",
            "                 works like the ``-z`` option described above, but it applies to\n",
            "                 all uploaded files, regardless of extension.\n",
            "\n",
            "                 CAUTION: If some of the source files don't compress well, such\n",
            "                 as binary data, using this option may result in files taking up\n",
            "                 more space in the cloud than they would if left uncompressed.\n",
            "\n",
            "  --stet         If the STET binary can be found in boto or PATH, cp will\n",
            "                 use the split-trust encryption tool for end-to-end encryption."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Delete users"
      ],
      "metadata": {
        "id": "ACiCkymh1FEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for uid in users_to_delete:\n",
        "    \n",
        "    print(uid)\n",
        "    response = sdk.user(\n",
        "            user_id= uid,\n",
        "            fields=\"email\")\n",
        "    print(response.email)   \n",
        "    \n",
        "    #delete users\n",
        "    # response = sdk.delete_user(uid)\n",
        "    # print(\"User successfully deleted\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ogl2KRIG1Heh",
        "outputId": "39f1fb9c-7918-4d76-d452-29bdb6af7b7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19371\n",
            "haengeun+teach+GlobalLiberty2@google.com\n",
            "19811\n",
            "karenpang@looker.com\n"
          ]
        }
      ]
    }
  ]
}